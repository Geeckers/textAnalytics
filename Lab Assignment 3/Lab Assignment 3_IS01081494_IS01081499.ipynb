{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6615ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dayang\n",
      "[nltk_data]     Nurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dayang\n",
      "[nltk_data]     Nurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dayang\n",
      "[nltk_data]     Nurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Dayang\n",
      "[nltk_data]     Nurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group member:\n",
    "#Dayang Nurin Syazwina Binti Ramlan (IS01081494)\n",
    "#Lina Batrisyia Binti Mohd Mazlan (IS01081499)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8971f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_dataset.csv')\n",
    "texts = df['text'].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec623244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic characters, lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the texts\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c671625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "# Filter out extremes to limit the number of features\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Create a corpus: Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc999287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for LDA\n",
    "num_topics = 4  # Number of topics\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=num_topics,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82371942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5347159504916545\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd4ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.065*\"db\" + 0.041*\"e\" + 0.034*\"q\" + 0.031*\"k\" + 0.029*\"n\" + 0.022*\"april\" + 0.022*\"x\" + 0.019*\"f\" + 0.015*\"b\" + 0.015*\"p\"')\n",
      "(1, '0.017*\"people\" + 0.015*\"government\" + 0.010*\"law\" + 0.008*\"u\" + 0.007*\"one\" + 0.007*\"state\" + 0.006*\"would\" + 0.006*\"right\" + 0.005*\"armenian\" + 0.005*\"say\"')\n",
      "(2, '0.019*\"would\" + 0.013*\"one\" + 0.012*\"like\" + 0.011*\"could\" + 0.011*\"know\" + 0.010*\"get\" + 0.008*\"time\" + 0.008*\"think\" + 0.007*\"good\" + 0.006*\"much\"')\n",
      "(3, '0.038*\"key\" + 0.019*\"chip\" + 0.016*\"encryption\" + 0.015*\"system\" + 0.014*\"use\" + 0.011*\"clipper\" + 0.009*\"information\" + 0.009*\"message\" + 0.009*\"phone\" + 0.008*\"algorithm\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The LDA model's topics highlight significant patterns throughout the dataset,\n",
    "#despite the existence of considerable noise in the first topic. \n",
    "#Topic 1 appears to concentrate on political and legal issues, as evidenced by phrases such as \"people,\" \"government,\" and \"law.\" \n",
    "#Topic 2 is more conversational and casual, including terms like \"would,\" \"like,\" and \"know,\" implying general talks or viewpoints.\n",
    "#Topic 3 focuses on technology and encryption, as seen by terminology like \"key,\" \"chip,\" and \"encryption.\" \n",
    "#The first subject, which contains seemingly random characters and symbols, might suggest preprocessing difficulties or non-standard content. \n",
    "#The coherence score of 0.5347 indicates a moderate amount of interpretability and coherence across the topics, \n",
    "#implying that while the model has caught some relevant themes, there is still potential for improvement, particularly in addressing data noise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
